{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs231n_lecture_04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 4 Backpropagation and Neural Networks"
      ],
      "metadata": {
        "id": "j1xGJO3g6q_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization"
      ],
      "metadata": {
        "id": "uWr2fjYZ6yN3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s9wvdVvnyZN5"
      },
      "outputs": [],
      "source": [
        "# while True:\n",
        "#     weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
        "#     weights += - step_size * weights_grad # parameter update"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modularized implementation: forward / backward API"
      ],
      "metadata": {
        "id": "QnLkt_JX-Ja1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # rough psuedo code\n",
        "class ComputationalGraph(object):\n",
        "    # ...\n",
        "    def forward(inputs):\n",
        "        # 1. [pass inputs to input gates...]\n",
        "        # 2. forward the compytational graph:\n",
        "        for gate in self.graph.nodes_topologically_sorted():\n",
        "            gate.forward()\n",
        "        return loss # the final gate in the graph outputs the loss\n",
        "\n",
        "    def backward():\n",
        "        for gate in reversed(self.graph.nodes_topologically_sorted()):\n",
        "            gate.backward() # little piece of backprop (chain rule applied)\n",
        "        return inputs_gradients"
      ],
      "metadata": {
        "id": "bOI1yNL67DON"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modularized implementation: forward / backward API"
      ],
      "metadata": {
        "id": "Magltzzt_BRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiplyGate(object):\n",
        "    def forward(x, y):\n",
        "        z = x*y\n",
        "        self.x = x # must keep these around! <- 기억해둬야 나중에 계산에서 쓸 수 있음\n",
        "        self.y = y\n",
        "        return z\n",
        "\n",
        "    def backward(dz):\n",
        "        dx = self.y * dz # [dz/dx * dL/dz]\n",
        "        dy = self.x * dz # [dz/dy * dL/dz]\n",
        "        return [dx, dy]"
      ],
      "metadata": {
        "id": "iwH8980d_AGC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full implementation of training a 2-layer Neural Network"
      ],
      "metadata": {
        "id": "1yDTXEbKBY2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 1000, 10\n",
        "x, y = randn(N, D_in), randn(N, D_out)\n",
        "w1, w2 = randn(D_in, H), randn(H, D_out)\n",
        "\n",
        "for t in range(2000):\n",
        "    h = 1 / (1 + np.exp(-x.dot(w1)))\n",
        "    y_pred = h.dot(w2)\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    if t % 100 == 0:\n",
        "        print(t, loss)\n",
        "\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w2 = h.T.dot(grad_y_pred)\n",
        "    grad_h = grad_y_pred.dot(w2.T)\n",
        "    grad_w1 = x.T.dot(grad_h * h * (1 - h))\n",
        "\n",
        "    w1 -= 1e-4 * grad_w1\n",
        "    w2 -= 1e-4 * grad_w2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4ACuafABipm",
        "outputId": "612e2160-71bc-42a5-8014-886780b7095f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 235490.4014824084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 1321.112230422572\n",
            "200 31.562827435615382\n",
            "300 1.1205131714576424\n",
            "400 0.04622141137564545\n",
            "500 0.0020297709365140687\n",
            "600 9.168514276452287e-05\n",
            "700 4.200543959663522e-06\n",
            "800 1.9398139802201755e-07\n",
            "900 9.002393326799017e-09\n",
            "1000 4.19204678471719e-10\n",
            "1100 1.957027816209907e-11\n",
            "1200 9.155031326679536e-13\n",
            "1300 4.290305317248883e-14\n",
            "1400 2.0137377913163287e-15\n",
            "1500 9.465675792155848e-17\n",
            "1600 4.455641408184428e-18\n",
            "1700 2.1004794005980451e-19\n",
            "1800 9.921871688110512e-21\n",
            "1900 4.719520920158549e-22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example feed-forward computation of a  neural network"
      ],
      "metadata": {
        "id": "iRze3xbsESqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Neuron:\n",
        "    # ...\n",
        "    def neuron_tick(inputs):\n",
        "        # assume inputs and weights are 1-D numpy arrays and bias is a number\n",
        "        cell_body_sum = np.sum(inputs * self.weights) + self.bias\n",
        "        firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation func <- sigmoid 함수 따로 만들어서 사용하는게 일반적일듯\n",
        "        return firing_rate"
      ],
      "metadata": {
        "id": "zWN7kv7KDkQE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## forward-pass of a 3-layer neural network:\n",
        "# f = lambda x: 1.0/(1.0 + np.exp(-x))\n",
        "# x = randn(3, 1)\n",
        "# h1 = f(np.dot(W1, x) + b1)\n",
        "# h2 = f(np.dot(W2, h1) + b2)\n",
        "# out = np.dot(W3, h2) + b3"
      ],
      "metadata": {
        "id": "fk-VKwMVE_u9"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}