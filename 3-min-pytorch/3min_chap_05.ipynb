{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3min_chap_05.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx-Nai6sV9Dj"
      },
      "source": [
        "# Chapter 05 이미지 처리 능력이 탁월한 CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiDEr4rYV3tO"
      },
      "source": [
        "## 5.2 CNN 모델 구현\n",
        "* Conv -> Pooling -> Conv -> Dropout -> Pooling -> Dense -> Dropout -> Dense 구조로 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2noZWiuJWN02"
      },
      "source": [
        "필요한 모듈 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p9ydzIpV4Xy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jL24rbsWdhO"
      },
      "source": [
        "CUDA 사용가능 여부"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tIxdjt_WaXi"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ZwWX_WYQa7"
      },
      "source": [
        "하이퍼파라미터인 이폭과 배치크기 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BDlXvOdYLcT"
      },
      "source": [
        "EPOCHS = 40\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlBtSwu7YaSb"
      },
      "source": [
        "데이터 불러오기 및 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQBen-lvYVxP"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.FashionMNIST('./.data',\n",
        "                          train=True,\n",
        "                          download=True,\n",
        "                          transform=transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))\n",
        "                          ])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.FashionMNIST('./.data',\n",
        "                          train=False,\n",
        "                          download=True,\n",
        "                          transform=transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))\n",
        "                          ])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmNYgSxWZeBC"
      },
      "source": [
        "* 만들 CNN 모델의 커널 크기는 5x5, 컨볼루션 계층 2개\n",
        "* nn.Conv2d 모듈은 자신을 바로 부를 수 있는 인스턴스, 그냥 함수로 생각해도 무방\n",
        "* 즉, self.conv1, self.conv2와 같은 CNN 모델의 내부 변수들은 함수로 취급될 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AddgyG_pZV6X"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # 채널 1, 커널 갯수 10, 커널 사이즈 5x5\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "\n",
        "        # 드롭아웃 함수 사용하지 않고 모듈로 드롭아웃 인스턴스 만듬\n",
        "        self.drop = nn.Dropout2d()\n",
        "\n",
        "        # 일반 신경망 정의\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    # 입력부터 출력까지의 데이터가 지나갈 길 만들기\n",
        "    def forward(self, x):\n",
        "        # Conv층\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))  # 두번째 입력은 커널 크기\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "\n",
        "        # 밀집층에 넣기위해 펼쳐줌\n",
        "        x = x.view(-1, 320)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXoHzDeVcJ4B"
      },
      "source": [
        "CNN 모델의 인스턴스와 최적화 함수 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOCY7ohVcIGB"
      },
      "source": [
        "model = CNN().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p_V-Fd9c-ax"
      },
      "source": [
        "모델 훈련 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OZazDq9crPA"
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}\\\n",
        "            ({100.*batch_idx/len(train_loader):.0f}%)]\\tLoss:{loss.item():.6f}\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDg4LKVielXM"
      },
      "source": [
        "평가 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UTr79-Aei3Y"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            # 배치 오차 합산\n",
        "            test_loss += F.cross_entropy(output, target,\n",
        "                                         reduction='sum').item()\n",
        "\n",
        "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdpaWPHnf980"
      },
      "source": [
        "코드 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU9TTbV0f8ig",
        "outputId": "4f4256d3-ff5e-47f8-b151-e357deed7870"
      },
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "    print(f'[{epoch}] Test Loss: {test_loss:4f}, Accuracy: {test_accuracy:.2f}%')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000            (0%)]\tLoss:2.310597\n",
            "Train Epoch: 1 [12800/60000            (21%)]\tLoss:1.036684\n",
            "Train Epoch: 1 [25600/60000            (43%)]\tLoss:1.214036\n",
            "Train Epoch: 1 [38400/60000            (64%)]\tLoss:0.804171\n",
            "Train Epoch: 1 [51200/60000            (85%)]\tLoss:0.719256\n",
            "[1] Test Loss: 0.626241, Accuracy: 76.19%\n",
            "Train Epoch: 2 [0/60000            (0%)]\tLoss:0.637757\n",
            "Train Epoch: 2 [12800/60000            (21%)]\tLoss:0.826681\n",
            "Train Epoch: 2 [25600/60000            (43%)]\tLoss:0.684175\n",
            "Train Epoch: 2 [38400/60000            (64%)]\tLoss:0.579217\n",
            "Train Epoch: 2 [51200/60000            (85%)]\tLoss:0.577356\n",
            "[2] Test Loss: 0.532268, Accuracy: 79.27%\n",
            "Train Epoch: 3 [0/60000            (0%)]\tLoss:0.549608\n",
            "Train Epoch: 3 [12800/60000            (21%)]\tLoss:0.619545\n",
            "Train Epoch: 3 [25600/60000            (43%)]\tLoss:0.587850\n",
            "Train Epoch: 3 [38400/60000            (64%)]\tLoss:0.935956\n",
            "Train Epoch: 3 [51200/60000            (85%)]\tLoss:0.646701\n",
            "[3] Test Loss: 0.493996, Accuracy: 82.32%\n",
            "Train Epoch: 4 [0/60000            (0%)]\tLoss:0.580318\n",
            "Train Epoch: 4 [12800/60000            (21%)]\tLoss:0.720775\n",
            "Train Epoch: 4 [25600/60000            (43%)]\tLoss:0.554922\n",
            "Train Epoch: 4 [38400/60000            (64%)]\tLoss:0.416502\n",
            "Train Epoch: 4 [51200/60000            (85%)]\tLoss:0.643043\n",
            "[4] Test Loss: 0.456888, Accuracy: 83.90%\n",
            "Train Epoch: 5 [0/60000            (0%)]\tLoss:0.622511\n",
            "Train Epoch: 5 [12800/60000            (21%)]\tLoss:0.779684\n",
            "Train Epoch: 5 [25600/60000            (43%)]\tLoss:0.476283\n",
            "Train Epoch: 5 [38400/60000            (64%)]\tLoss:0.595245\n",
            "Train Epoch: 5 [51200/60000            (85%)]\tLoss:0.483470\n",
            "[5] Test Loss: 0.421468, Accuracy: 85.07%\n",
            "Train Epoch: 6 [0/60000            (0%)]\tLoss:0.702168\n",
            "Train Epoch: 6 [12800/60000            (21%)]\tLoss:0.534324\n",
            "Train Epoch: 6 [25600/60000            (43%)]\tLoss:0.485923\n",
            "Train Epoch: 6 [38400/60000            (64%)]\tLoss:0.615102\n",
            "Train Epoch: 6 [51200/60000            (85%)]\tLoss:0.466938\n",
            "[6] Test Loss: 0.402588, Accuracy: 85.35%\n",
            "Train Epoch: 7 [0/60000            (0%)]\tLoss:0.411407\n",
            "Train Epoch: 7 [12800/60000            (21%)]\tLoss:0.519401\n",
            "Train Epoch: 7 [25600/60000            (43%)]\tLoss:0.759830\n",
            "Train Epoch: 7 [38400/60000            (64%)]\tLoss:0.445197\n",
            "Train Epoch: 7 [51200/60000            (85%)]\tLoss:0.589773\n",
            "[7] Test Loss: 0.391149, Accuracy: 85.97%\n",
            "Train Epoch: 8 [0/60000            (0%)]\tLoss:0.592083\n",
            "Train Epoch: 8 [12800/60000            (21%)]\tLoss:0.398768\n",
            "Train Epoch: 8 [25600/60000            (43%)]\tLoss:0.330737\n",
            "Train Epoch: 8 [38400/60000            (64%)]\tLoss:0.466865\n",
            "Train Epoch: 8 [51200/60000            (85%)]\tLoss:0.485963\n",
            "[8] Test Loss: 0.374953, Accuracy: 86.36%\n",
            "Train Epoch: 9 [0/60000            (0%)]\tLoss:0.362549\n",
            "Train Epoch: 9 [12800/60000            (21%)]\tLoss:0.381357\n",
            "Train Epoch: 9 [25600/60000            (43%)]\tLoss:0.504607\n",
            "Train Epoch: 9 [38400/60000            (64%)]\tLoss:0.328428\n",
            "Train Epoch: 9 [51200/60000            (85%)]\tLoss:0.340476\n",
            "[9] Test Loss: 0.368853, Accuracy: 86.71%\n",
            "Train Epoch: 10 [0/60000            (0%)]\tLoss:0.406865\n",
            "Train Epoch: 10 [12800/60000            (21%)]\tLoss:0.455821\n",
            "Train Epoch: 10 [25600/60000            (43%)]\tLoss:0.509723\n",
            "Train Epoch: 10 [38400/60000            (64%)]\tLoss:0.307771\n",
            "Train Epoch: 10 [51200/60000            (85%)]\tLoss:0.308408\n",
            "[10] Test Loss: 0.353006, Accuracy: 87.47%\n",
            "Train Epoch: 11 [0/60000            (0%)]\tLoss:0.522806\n",
            "Train Epoch: 11 [12800/60000            (21%)]\tLoss:0.534203\n",
            "Train Epoch: 11 [25600/60000            (43%)]\tLoss:0.339457\n",
            "Train Epoch: 11 [38400/60000            (64%)]\tLoss:0.319855\n",
            "Train Epoch: 11 [51200/60000            (85%)]\tLoss:0.450512\n",
            "[11] Test Loss: 0.350171, Accuracy: 87.32%\n",
            "Train Epoch: 12 [0/60000            (0%)]\tLoss:0.204200\n",
            "Train Epoch: 12 [12800/60000            (21%)]\tLoss:0.528813\n",
            "Train Epoch: 12 [25600/60000            (43%)]\tLoss:0.199027\n",
            "Train Epoch: 12 [38400/60000            (64%)]\tLoss:0.455294\n",
            "Train Epoch: 12 [51200/60000            (85%)]\tLoss:0.400418\n",
            "[12] Test Loss: 0.345960, Accuracy: 87.92%\n",
            "Train Epoch: 13 [0/60000            (0%)]\tLoss:0.611793\n",
            "Train Epoch: 13 [12800/60000            (21%)]\tLoss:0.620881\n",
            "Train Epoch: 13 [25600/60000            (43%)]\tLoss:0.442039\n",
            "Train Epoch: 13 [38400/60000            (64%)]\tLoss:0.307948\n",
            "Train Epoch: 13 [51200/60000            (85%)]\tLoss:0.356828\n",
            "[13] Test Loss: 0.329854, Accuracy: 87.82%\n",
            "Train Epoch: 14 [0/60000            (0%)]\tLoss:0.502285\n",
            "Train Epoch: 14 [12800/60000            (21%)]\tLoss:0.383042\n",
            "Train Epoch: 14 [25600/60000            (43%)]\tLoss:0.216776\n",
            "Train Epoch: 14 [38400/60000            (64%)]\tLoss:0.482682\n",
            "Train Epoch: 14 [51200/60000            (85%)]\tLoss:0.406962\n",
            "[14] Test Loss: 0.329779, Accuracy: 88.19%\n",
            "Train Epoch: 15 [0/60000            (0%)]\tLoss:0.348642\n",
            "Train Epoch: 15 [12800/60000            (21%)]\tLoss:0.329810\n",
            "Train Epoch: 15 [25600/60000            (43%)]\tLoss:0.395609\n",
            "Train Epoch: 15 [38400/60000            (64%)]\tLoss:0.266631\n",
            "Train Epoch: 15 [51200/60000            (85%)]\tLoss:0.314655\n",
            "[15] Test Loss: 0.331546, Accuracy: 88.23%\n",
            "Train Epoch: 16 [0/60000            (0%)]\tLoss:0.230331\n",
            "Train Epoch: 16 [12800/60000            (21%)]\tLoss:0.469336\n",
            "Train Epoch: 16 [25600/60000            (43%)]\tLoss:0.616429\n",
            "Train Epoch: 16 [38400/60000            (64%)]\tLoss:0.297419\n",
            "Train Epoch: 16 [51200/60000            (85%)]\tLoss:0.365960\n",
            "[16] Test Loss: 0.333916, Accuracy: 88.03%\n",
            "Train Epoch: 17 [0/60000            (0%)]\tLoss:0.405850\n",
            "Train Epoch: 17 [12800/60000            (21%)]\tLoss:0.362881\n",
            "Train Epoch: 17 [25600/60000            (43%)]\tLoss:0.642603\n",
            "Train Epoch: 17 [38400/60000            (64%)]\tLoss:0.416507\n",
            "Train Epoch: 17 [51200/60000            (85%)]\tLoss:0.410266\n",
            "[17] Test Loss: 0.320396, Accuracy: 88.24%\n",
            "Train Epoch: 18 [0/60000            (0%)]\tLoss:0.277531\n",
            "Train Epoch: 18 [12800/60000            (21%)]\tLoss:0.374021\n",
            "Train Epoch: 18 [25600/60000            (43%)]\tLoss:0.282375\n",
            "Train Epoch: 18 [38400/60000            (64%)]\tLoss:0.373377\n",
            "Train Epoch: 18 [51200/60000            (85%)]\tLoss:0.465218\n",
            "[18] Test Loss: 0.311278, Accuracy: 88.84%\n",
            "Train Epoch: 19 [0/60000            (0%)]\tLoss:0.321530\n",
            "Train Epoch: 19 [12800/60000            (21%)]\tLoss:0.276241\n",
            "Train Epoch: 19 [25600/60000            (43%)]\tLoss:0.348196\n",
            "Train Epoch: 19 [38400/60000            (64%)]\tLoss:0.293564\n",
            "Train Epoch: 19 [51200/60000            (85%)]\tLoss:0.385164\n",
            "[19] Test Loss: 0.315168, Accuracy: 88.56%\n",
            "Train Epoch: 20 [0/60000            (0%)]\tLoss:0.361770\n",
            "Train Epoch: 20 [12800/60000            (21%)]\tLoss:0.298443\n",
            "Train Epoch: 20 [25600/60000            (43%)]\tLoss:0.300696\n",
            "Train Epoch: 20 [38400/60000            (64%)]\tLoss:0.444014\n",
            "Train Epoch: 20 [51200/60000            (85%)]\tLoss:0.236043\n",
            "[20] Test Loss: 0.302553, Accuracy: 89.13%\n",
            "Train Epoch: 21 [0/60000            (0%)]\tLoss:0.371325\n",
            "Train Epoch: 21 [12800/60000            (21%)]\tLoss:0.583911\n",
            "Train Epoch: 21 [25600/60000            (43%)]\tLoss:0.217266\n",
            "Train Epoch: 21 [38400/60000            (64%)]\tLoss:0.380570\n",
            "Train Epoch: 21 [51200/60000            (85%)]\tLoss:0.550033\n",
            "[21] Test Loss: 0.298838, Accuracy: 89.51%\n",
            "Train Epoch: 22 [0/60000            (0%)]\tLoss:0.281011\n",
            "Train Epoch: 22 [12800/60000            (21%)]\tLoss:0.259466\n",
            "Train Epoch: 22 [25600/60000            (43%)]\tLoss:0.338622\n",
            "Train Epoch: 22 [38400/60000            (64%)]\tLoss:0.275294\n",
            "Train Epoch: 22 [51200/60000            (85%)]\tLoss:0.264729\n",
            "[22] Test Loss: 0.307221, Accuracy: 88.97%\n",
            "Train Epoch: 23 [0/60000            (0%)]\tLoss:0.234942\n",
            "Train Epoch: 23 [12800/60000            (21%)]\tLoss:0.316434\n",
            "Train Epoch: 23 [25600/60000            (43%)]\tLoss:0.592278\n",
            "Train Epoch: 23 [38400/60000            (64%)]\tLoss:0.127059\n",
            "Train Epoch: 23 [51200/60000            (85%)]\tLoss:0.254583\n",
            "[23] Test Loss: 0.305540, Accuracy: 88.82%\n",
            "Train Epoch: 24 [0/60000            (0%)]\tLoss:0.321591\n",
            "Train Epoch: 24 [12800/60000            (21%)]\tLoss:0.287133\n",
            "Train Epoch: 24 [25600/60000            (43%)]\tLoss:0.245711\n",
            "Train Epoch: 24 [38400/60000            (64%)]\tLoss:0.208783\n",
            "Train Epoch: 24 [51200/60000            (85%)]\tLoss:0.335808\n",
            "[24] Test Loss: 0.301502, Accuracy: 89.03%\n",
            "Train Epoch: 25 [0/60000            (0%)]\tLoss:0.329794\n",
            "Train Epoch: 25 [12800/60000            (21%)]\tLoss:0.457893\n",
            "Train Epoch: 25 [25600/60000            (43%)]\tLoss:0.251895\n",
            "Train Epoch: 25 [38400/60000            (64%)]\tLoss:0.195897\n",
            "Train Epoch: 25 [51200/60000            (85%)]\tLoss:0.290600\n",
            "[25] Test Loss: 0.297472, Accuracy: 89.24%\n",
            "Train Epoch: 26 [0/60000            (0%)]\tLoss:0.365341\n",
            "Train Epoch: 26 [12800/60000            (21%)]\tLoss:0.278782\n",
            "Train Epoch: 26 [25600/60000            (43%)]\tLoss:0.254130\n",
            "Train Epoch: 26 [38400/60000            (64%)]\tLoss:0.306808\n",
            "Train Epoch: 26 [51200/60000            (85%)]\tLoss:0.338117\n",
            "[26] Test Loss: 0.292232, Accuracy: 89.22%\n",
            "Train Epoch: 27 [0/60000            (0%)]\tLoss:0.253239\n",
            "Train Epoch: 27 [12800/60000            (21%)]\tLoss:0.337781\n",
            "Train Epoch: 27 [25600/60000            (43%)]\tLoss:0.286152\n",
            "Train Epoch: 27 [38400/60000            (64%)]\tLoss:0.207925\n",
            "Train Epoch: 27 [51200/60000            (85%)]\tLoss:0.163237\n",
            "[27] Test Loss: 0.297259, Accuracy: 89.37%\n",
            "Train Epoch: 28 [0/60000            (0%)]\tLoss:0.222222\n",
            "Train Epoch: 28 [12800/60000            (21%)]\tLoss:0.194629\n",
            "Train Epoch: 28 [25600/60000            (43%)]\tLoss:0.351159\n",
            "Train Epoch: 28 [38400/60000            (64%)]\tLoss:0.238692\n",
            "Train Epoch: 28 [51200/60000            (85%)]\tLoss:0.342519\n",
            "[28] Test Loss: 0.303896, Accuracy: 89.19%\n",
            "Train Epoch: 29 [0/60000            (0%)]\tLoss:0.210303\n",
            "Train Epoch: 29 [12800/60000            (21%)]\tLoss:0.446872\n",
            "Train Epoch: 29 [25600/60000            (43%)]\tLoss:0.490521\n",
            "Train Epoch: 29 [38400/60000            (64%)]\tLoss:0.244828\n",
            "Train Epoch: 29 [51200/60000            (85%)]\tLoss:0.286915\n",
            "[29] Test Loss: 0.285817, Accuracy: 89.66%\n",
            "Train Epoch: 30 [0/60000            (0%)]\tLoss:0.286723\n",
            "Train Epoch: 30 [12800/60000            (21%)]\tLoss:0.191809\n",
            "Train Epoch: 30 [25600/60000            (43%)]\tLoss:0.211382\n",
            "Train Epoch: 30 [38400/60000            (64%)]\tLoss:0.202144\n",
            "Train Epoch: 30 [51200/60000            (85%)]\tLoss:0.371288\n",
            "[30] Test Loss: 0.292027, Accuracy: 89.50%\n",
            "Train Epoch: 31 [0/60000            (0%)]\tLoss:0.278256\n",
            "Train Epoch: 31 [12800/60000            (21%)]\tLoss:0.254663\n",
            "Train Epoch: 31 [25600/60000            (43%)]\tLoss:0.370587\n",
            "Train Epoch: 31 [38400/60000            (64%)]\tLoss:0.247489\n",
            "Train Epoch: 31 [51200/60000            (85%)]\tLoss:0.413815\n",
            "[31] Test Loss: 0.300999, Accuracy: 89.15%\n",
            "Train Epoch: 32 [0/60000            (0%)]\tLoss:0.454057\n",
            "Train Epoch: 32 [12800/60000            (21%)]\tLoss:0.301795\n",
            "Train Epoch: 32 [25600/60000            (43%)]\tLoss:0.323843\n",
            "Train Epoch: 32 [38400/60000            (64%)]\tLoss:0.345600\n",
            "Train Epoch: 32 [51200/60000            (85%)]\tLoss:0.309676\n",
            "[32] Test Loss: 0.290464, Accuracy: 89.34%\n",
            "Train Epoch: 33 [0/60000            (0%)]\tLoss:0.335283\n",
            "Train Epoch: 33 [12800/60000            (21%)]\tLoss:0.186998\n",
            "Train Epoch: 33 [25600/60000            (43%)]\tLoss:0.221168\n",
            "Train Epoch: 33 [38400/60000            (64%)]\tLoss:0.378683\n",
            "Train Epoch: 33 [51200/60000            (85%)]\tLoss:0.270611\n",
            "[33] Test Loss: 0.290594, Accuracy: 89.75%\n",
            "Train Epoch: 34 [0/60000            (0%)]\tLoss:0.383828\n",
            "Train Epoch: 34 [12800/60000            (21%)]\tLoss:0.172341\n",
            "Train Epoch: 34 [25600/60000            (43%)]\tLoss:0.262689\n",
            "Train Epoch: 34 [38400/60000            (64%)]\tLoss:0.385342\n",
            "Train Epoch: 34 [51200/60000            (85%)]\tLoss:0.402890\n",
            "[34] Test Loss: 0.282908, Accuracy: 89.89%\n",
            "Train Epoch: 35 [0/60000            (0%)]\tLoss:0.331179\n",
            "Train Epoch: 35 [12800/60000            (21%)]\tLoss:0.347668\n",
            "Train Epoch: 35 [25600/60000            (43%)]\tLoss:0.322288\n",
            "Train Epoch: 35 [38400/60000            (64%)]\tLoss:0.224418\n",
            "Train Epoch: 35 [51200/60000            (85%)]\tLoss:0.377426\n",
            "[35] Test Loss: 0.292605, Accuracy: 89.41%\n",
            "Train Epoch: 36 [0/60000            (0%)]\tLoss:0.348009\n",
            "Train Epoch: 36 [12800/60000            (21%)]\tLoss:0.214531\n",
            "Train Epoch: 36 [25600/60000            (43%)]\tLoss:0.202971\n",
            "Train Epoch: 36 [38400/60000            (64%)]\tLoss:0.324101\n",
            "Train Epoch: 36 [51200/60000            (85%)]\tLoss:0.348771\n",
            "[36] Test Loss: 0.291198, Accuracy: 89.65%\n",
            "Train Epoch: 37 [0/60000            (0%)]\tLoss:0.384904\n",
            "Train Epoch: 37 [12800/60000            (21%)]\tLoss:0.186254\n",
            "Train Epoch: 37 [25600/60000            (43%)]\tLoss:0.249902\n",
            "Train Epoch: 37 [38400/60000            (64%)]\tLoss:0.487680\n",
            "Train Epoch: 37 [51200/60000            (85%)]\tLoss:0.217962\n",
            "[37] Test Loss: 0.278305, Accuracy: 90.21%\n",
            "Train Epoch: 38 [0/60000            (0%)]\tLoss:0.309190\n",
            "Train Epoch: 38 [12800/60000            (21%)]\tLoss:0.270428\n",
            "Train Epoch: 38 [25600/60000            (43%)]\tLoss:0.423657\n",
            "Train Epoch: 38 [38400/60000            (64%)]\tLoss:0.270719\n",
            "Train Epoch: 38 [51200/60000            (85%)]\tLoss:0.463470\n",
            "[38] Test Loss: 0.290414, Accuracy: 89.45%\n",
            "Train Epoch: 39 [0/60000            (0%)]\tLoss:0.404480\n",
            "Train Epoch: 39 [12800/60000            (21%)]\tLoss:0.246237\n",
            "Train Epoch: 39 [25600/60000            (43%)]\tLoss:0.440040\n",
            "Train Epoch: 39 [38400/60000            (64%)]\tLoss:0.254922\n",
            "Train Epoch: 39 [51200/60000            (85%)]\tLoss:0.250148\n",
            "[39] Test Loss: 0.288292, Accuracy: 89.96%\n",
            "Train Epoch: 40 [0/60000            (0%)]\tLoss:0.348459\n",
            "Train Epoch: 40 [12800/60000            (21%)]\tLoss:0.381607\n",
            "Train Epoch: 40 [25600/60000            (43%)]\tLoss:0.277219\n",
            "Train Epoch: 40 [38400/60000            (64%)]\tLoss:0.196439\n",
            "Train Epoch: 40 [51200/60000            (85%)]\tLoss:0.378263\n",
            "[40] Test Loss: 0.274870, Accuracy: 90.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hexnASrHjENl"
      },
      "source": [
        "### 전체 코드 쳐보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjEniOE6gXXA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "\n",
        "EPOCHS = 40\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./.data',\n",
        "                   train=True,\n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size = BATCH_SIZE, shuffle=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./.data',\n",
        "                   train=True,\n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3801,))\n",
        "                   ])),\n",
        "    batch_size = BATCH_SIZE, shuffle=True\n",
        ")\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = Net().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            # 배치 오차 합산\n",
        "            test_loss += F.cross_entropy(output, target,\n",
        "                                         reduction='sum').item()\n",
        "            \n",
        "            # 가장 높은 값을 가진 인덱스가 예측값임\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "    print(f'[{epoch}] Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pJrs04znqpu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}